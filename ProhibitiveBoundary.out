\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Reinforcement Learning For Control Researchers}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{A Note on Notations}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{From Approximate Dynamic Programming to RL}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Combine Policy Approximation and Cost-to-go Approximation}{section.2}% 5
\BOOKMARK [3][-]{subsubsection.2.3.1}{Advantage Actor Critic\(A2C\)}{subsection.2.3}% 6
\BOOKMARK [3][-]{subsubsection.2.3.2}{Trust Region Actor Critic}{subsection.2.3}% 7
\BOOKMARK [3][-]{subsubsection.2.3.3}{PPO}{subsection.2.3}% 8
\BOOKMARK [2][-]{subsection.2.4}{Entropy Regularization}{section.2}% 9
\BOOKMARK [1][-]{section.3}{Protective Boundary}{}% 10
\BOOKMARK [2][-]{subsection.3.1}{Protective Boundaries in Atheletic Training}{section.3}% 11
\BOOKMARK [2][-]{subsection.3.2}{Implement Protective Boundary in RL Setup}{section.3}% 12
\BOOKMARK [2][-]{subsection.3.3}{Experiments on OpenAI Gym}{section.3}% 13
\BOOKMARK [3][-]{subsubsection.3.3.1}{CartPole}{subsection.3.3}% 14
\BOOKMARK [3][-]{subsubsection.3.3.2}{Inverted Pendulum}{subsection.3.3}% 15
\BOOKMARK [3][-]{subsubsection.3.3.3}{Inverted Double Pendulum}{subsection.3.3}% 16
\BOOKMARK [3][-]{subsubsection.3.3.4}{Walker2d}{subsection.3.3}% 17
\BOOKMARK [3][-]{subsubsection.3.3.5}{Hopper}{subsection.3.3}% 18
\BOOKMARK [3][-]{subsubsection.3.3.6}{}{subsection.3.3}% 19
\BOOKMARK [1][-]{section.4}{Conclution}{}% 20
\BOOKMARK [1][-]{section*.8}{References}{}% 21
