\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Reinforcement Learning For Control Researchers}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{A Note on Notations}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{From Approximate Dynamic Programming to RL}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Combine Policy Approximation and Cost-to-go Approximation}{section.2}% 5
\BOOKMARK [3][-]{subsubsection.2.3.1}{Advantage Actor Critic\(A2C\)}{subsection.2.3}% 6
\BOOKMARK [3][-]{subsubsection.2.3.2}{Trust Region Actor Critic}{subsection.2.3}% 7
\BOOKMARK [3][-]{subsubsection.2.3.3}{PPO}{subsection.2.3}% 8
\BOOKMARK [2][-]{subsection.2.4}{Entropy Regularization}{section.2}% 9
\BOOKMARK [1][-]{section.3}{Protective Boundary}{}% 10
\BOOKMARK [2][-]{subsection.3.1}{Protective Boundaries in Atheletic Training}{section.3}% 11
\BOOKMARK [2][-]{subsection.3.2}{Implement Protective Boundary in OpenAI Gym}{section.3}% 12
\BOOKMARK [2][-]{subsection.3.3}{CartPole and Inveretd Pendulum}{section.3}% 13
\BOOKMARK [2][-]{subsection.3.4}{Inverted Double Pendulum}{section.3}% 14
\BOOKMARK [3][-]{subsubsection.3.4.1}{Hopper}{subsection.3.4}% 15
\BOOKMARK [3][-]{subsubsection.3.4.2}{Walker2d}{subsection.3.4}% 16
\BOOKMARK [1][-]{section.4}{Conclution}{}% 17
\BOOKMARK [1][-]{section*.10}{References}{}% 18
