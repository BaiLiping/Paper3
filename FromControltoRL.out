\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Where to Approximate}{}% 2
\BOOKMARK [1][-]{section.3}{System Modelling}{}% 3
\BOOKMARK [1][-]{section.4}{Policy Gradient}{}% 4
<<<<<<< HEAD
\BOOKMARK [1][-]{section.5}{Approximate Dynamic Programming}{}% 5
\BOOKMARK [2][-]{subsection.5.1}{Value Iteration and Policy Iteration}{section.5}% 6
\BOOKMARK [2][-]{subsection.5.2}{Maximum Entropy Reinforcement Learning}{section.5}% 7
\BOOKMARK [1][-]{section.6}{Other Ways to Utilize Increased Computational Power}{}% 8
\BOOKMARK [2][-]{subsection.6.1}{Multiagent Formulation}{section.6}% 9
\BOOKMARK [2][-]{subsection.6.2}{Random Shoot}{section.6}% 10
\BOOKMARK [1][-]{section*.4}{References}{}% 11
=======
\BOOKMARK [2][-]{subsection.4.1}{Trust Region Policy Iteration}{section.4}% 5
\BOOKMARK [2][-]{subsection.4.2}{Direct Policy Fitting}{section.4}% 6
\BOOKMARK [1][-]{section.5}{Approximate Dynamic Programming}{}% 7
\BOOKMARK [2][-]{subsection.5.1}{Value Iteration and Policy Iteration}{section.5}% 8
\BOOKMARK [2][-]{subsection.5.2}{Maximum Entropy Reinforcement Learning}{section.5}% 9
\BOOKMARK [1][-]{section.6}{Other Ways to Utilize Increased Computational Power}{}% 10
\BOOKMARK [2][-]{subsection.6.1}{Multiagent Formulation}{section.6}% 11
\BOOKMARK [2][-]{subsection.6.2}{Monte Carlo Tree Search}{section.6}% 12
\BOOKMARK [2][-]{subsection.6.3}{Random Shoot}{section.6}% 13
\BOOKMARK [1][-]{section*.8}{References}{}% 14
>>>>>>> 0a11db63e0e342d29b408b28c28e9214f7e293e2
